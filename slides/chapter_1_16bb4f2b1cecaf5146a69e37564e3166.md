---
title: Insert title here
key: 16bb4f2b1cecaf5146a69e37564e3166

---
## Bag of Words Pipeline

```yaml
type: "TitleSlide"
key: "639ff16d6a"
```

`@lower_third`

name: Prateek Narang
title: Machine Learning Researcher


`@script`



---
## Understanding Bag of Words(BOW) Model


```yaml
type: "FullSlide"
key: "14ea8de340"
```

`@part1`
Bag of Words Model is way of representing a document as collection of its words. Frequency of occurrence of each word acts helps to define the feature vector for the sentence/document.

**Assumption **- Order of occurrence of each single word in the sentence is not important! {{1}}


**Applications** {{2}}

- Classifying Text/Documents{{3}}
- Clustering Similar Documents{{4}}
- Sentiment/Tweet Analysis{{5}}
- Information Retrieval{{6}}


`@script`



---
## Let us see one Example

```yaml
type: "FullSlide"
key: "03fc4ee4df"
```

`@part1`
Suppose we got few phone product reviews, and we want to predict whether the review is positive or negative.

**Review 1** {{1}}
> I just **loved** the iPhone. The battery backup is very **good**, and display is excellent too. {{2}}

**Review 2**{{3}}
> 
This phone works very **good**, display is nice and build is also **good**. I **love **Apple devices.{{4}}


`@script`



---
## Bag of Words Assumption looks reasonable!

```yaml
type: "FullSlide"
key: "a2b7338bcb"
```

`@part1`
- The words "love", "loved", "good" appear at different indices but convey the same positive sentiment about the product. 
 
- So,BOW claims the feature vectors we are going to construct will be related to word frequencies and local positioning of words will be irrelevant.


`@script`



---
## Potential Problems with Assumption

```yaml
type: "FullSlide"
key: "1b84ad4817"
```

`@part1`
**Handling Negation**

Consider the following review  
  

> I didn't **like** the phone. The battery was not **good** 

Unigram Features - "good" ,"like", "phone" etc
 
**Solution**


Adjacent Pair of words "didn't_like", "not_good" can be used as features instead of single word. These are called **Bigrams** features.


`@script`
Now it contains features like "like", "good" but the overall sentiment is negative about the product. But from the examples we have seen earlier, our model is likely to see above review as positive because of highly positive feature words like - "like", "love", "good". We will learn to handle such cases later.


---
## Bag of Words Pipeline

```yaml
type: "FullSlide"
key: "95e8e6b186"
```

`@part1`
Our objective now is to convert a text document into numerical features. A common bag of words pipeline includes following steps.


**Steps Involved**
- Data Collection(Corpus)
- Tokenization & Stopword Removal
- Stemming/Lemmatization
- Building a Common Vocab
- Classification or Clustering


`@script`



---
## Tokenization

```yaml
type: "FullSlide"
key: "b374098c37"
```

`@part1`
Tokenization is process of breaking a document into sentences, and sentences into words.

Sample Text

On Sunny Days, I like to play Cricket.{{1}}

Tokenized Text

 
"On","Sunny","Days","I","like","to","play","Cricket" {{2}}


`@script`



---
## ```split()``` Function

```yaml
type: "FullCodeSlide"
key: "68130276a0"
disable_transition: false
```

`@part1`
Sample Text

You can use Python's split function to do the splitting.

```
l = "On Sunny Days, I like to play Cricket."
l = l.split()
``` {{1}}
Tokenized Text

 ```
"On","Sunny","Days","I","like","to","play","Cricket"
```{{2}}


`@script`



---
## Stopword Removal

```yaml
type: "FullSlide"
key: "8595f265a1"
```

`@part1`
We can see commonly occuring words like articles, prepositions, determines etc can be removed because they don't convey anything signifcant. So we removed these unwanted words called stopwords in the next step.

Tokenized Text

> 
"On","Sunny","Days","I","like","to","play","Cricket"
Cleaned Text


(after stopword removal & converting to lowercase)

> 
"sunny","days","like,"play","cricket"


`@script`



---
## NLTK Stopwords

```yaml
type: "FullCodeSlide"
key: "a691d8f039"
```

`@part1`
NLTK comes with a predefined set of Stopwords.
```
from nltk.corpus import stopwords

set(stopwords.words('english'))
```
{{1}}

Some sample stopwords

{‘ourselves’, ‘hers’, ‘between’, ‘yourself’, ‘but’, ‘again’, ‘there’, ‘about’, ‘once’.....}  {{2}}


`@script`



---
## Stemming

```yaml
type: "FullSlide"
key: "b2131b4271"
```

`@part1`
Many words(such as verbs) are derived from same base word, and we can reduce all derived/inflected forms of the word into base form in order to reduce total of unique features(words) for classification. This step is called stemming.

> 
John is the **tallest** guy in the class. He is 6 feet **tall** and 2 feet **taller** than me.  

Here "tall", "taller", "tallest" all represent the similar context. So it better to reduce of these words into base form word - **tall.**


`@script`



---
## Building a Common Vocabulary

```yaml
type: "FullSlide"
key: "4073079439"
```

`@part1`
After cleaning all documents, we can filter out all unique words which can be used as features. We combine these words into a single list called Vocabulary.

- Each word in the vocabulary is assigned a unique index, which represents the index of the word in the feature vector and frequency of the word as it value.

- The size of vocabulary is denoted by |V| and is also the size of feature vector for each word.


`@script`



---
## Vectorization

```yaml
type: "FullSlide"
key: "de50e1536e"
```

`@part1`
This is the last step of the pipeline. We use Vocabulary look up table to convert text into numerical feature vectors -

**Frequency Based Feature Vector** 
For each sentence/document we construct a feature vector having length equal to vocab size. 

> [ 1 0 3 4 2 0 0 0 1 0 ..........V ]

Ith element in the vector represents frequency of ith word from vocab.




We will use sklearn's count vectorizer class for the above purpose.

```
from sklearn.feature_extraction.text import CountVectorizer 
```
{{1}}


`@script`



---
## Let's practice

```yaml
type: "FinalSlide"
key: "0e2de7bb86"
```

`@script`


