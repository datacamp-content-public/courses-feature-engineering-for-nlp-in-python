---
title: Insert title here
key: 16bb4f2b1cecaf5146a69e37564e3166

---
## Bag of Words Pipeline

```yaml
type: "TitleSlide"
key: "639ff16d6a"
```

`@lower_third`

name: Prateek Narang
title: Instructor & Co-founder, Coding Blocks


`@script`
In the tutorial, you will understand the Bag of Words Pipeline, and steps involved in converting text data into numerical features.


---
## Understanding Bag of Words(BOW) Model

```yaml
type: "FullSlide"
key: "14ea8de340"
```

`@part1`
Bag of Words Model is way of representing a document as collection of its words. Frequency of occurrence of each word help to define the feature vector for a document. {{1}}

**Assumption **- Order of occurrence of each single word in the sentence is not important! {{2}}


**Applications** {{3}}

- Classifying Text/Documents{{3}}
- Clustering Similar Documents{{4}}
- Sentiment/Tweet Analysis{{5}}
- Information Retrieval{{6}}


`@script`
Bag of Words Model is way of representing a document as collection of its words. Frequency of occurrence of each word help to define the feature vector for a document.
It is based on the assumption that Order of occurrence of each single word in the sentence is not important.
It can be useful in tasks like Text Classification,Clustering, Senitment Analysis.


---
## Example

```yaml
type: "FullSlide"
key: "03fc4ee4df"
```

`@part1`
Suppose we got few phone product reviews, and we want to predict whether the review is positive or negative.

**Review 1** {{1}}
> I just **loved** the iPhone. The battery backup is very **good**. {{2}}

**Review 2**{{3}}
> 
This phone works very **good**, **loved** the display and build is also **good**.{{4}}


`@script`
Suppose we got some product reviews, and we want to predict if the review is positive or negative.
Here are the reviews.
Both reviews contains words like “good”,”love”,”loved” which indicate that the sentiment is positive.


---
## Bag of Words Assumption looks reasonable!

```yaml
type: "FullSlide"
key: "a2b7338bcb"
```

`@part1`
- The words "love","good" appear at different indices but convey the same positive sentiment about the product. {{1}}
 
- So,BOW assumption that the feature vectors are related to word frequencies and not to local positioning of words holds true.{{2}}


`@script`
We also observe that these words appear at different positions in the sentence but convey the same positive sentiment about the product. So,BOW assumption that the feature vectors are related to word frequencies and not to word positioning holds true.


---
## Potential Problems with Assumption

```yaml
type: "FullSlide"
key: "1b84ad4817"
```

`@part1`
**Handling Negation**

Consider the following review  {{1}}
  
>I didn't **like** the phone. The battery was not **good** {{1}}

Unigram Features - "good" ,"like", "phone" etc {{2}}
 
**Solution** {{3}}

Two Consecutive words "didn't_like", "not_good" can be used as features instead of single word. These are called **Bigram** features.{{4}}


`@script`
But there can be some potential problems with this assumption. You can see the given review contains features like "like", "good" which indicate positive sentiment but overall sentiment is negative.
 
To handle this, we introduce bigram features which means we will treat two consecutive words as a single feature.


---
## Bag of Words Pipeline

```yaml
type: "FullSlide"
key: "95e8e6b186"
```

`@part1`
Our objective now is to convert a text document into numerical features. A common bag of words pipeline includes following steps.


**Steps Involved**
- Data Collection(Corpus) {{1}}
- Tokenization & Stopword Removal {{2}}
- Stemming/Lemmatization {{3}}
- Building a Common Vocab {{4}}
- Vectorization {{5}}
- Classification or Clustering  {{6}}


`@script`
So let us now talk about Bag of Words Pipeline. 
Our objective now is to convert a text document into numerical features. Bag of words pipeline includes following steps.

We will start with data collection, followed by Tokenization, Stopwords Removal and Stemming, and then building a common vocabulary,finally vectorization before feeding into to our classifier. Let us understand these steps one by one.


---
## Tokenization

```yaml
type: "FullSlide"
key: "b374098c37"
```

`@part1`
Tokenization is process of breaking a document into smaller entities such as sentences and words.

**Sample Text** {{1}}

On Sunny Days, I like to play Cricket.{{2}}

**Tokenized Text** {{3}}

 
"On","Sunny","Days","I","like","to","play","Cricket" {{4}}


`@script`
Tokenization is process of breaking a document into into smaller units such as sentences and words. Let us break this sample text into words.


---
## split() Function

```yaml
type: "FullCodeSlide"
key: "68130276a0"
disable_transition: false
```

`@part1`
You can use Python's split function to do the splitting.

```
l = "On Sunny Days, I like to play Cricket."
l = l.split()
``` {{1}}

Tokenized Text {{2}}

 ```
"On","Sunny","Days","I","like","to","play","Cricket"
```{{2}}


`@script`
You can use Python's split function to do the splitting. We will explore more NLTK functions for splitting later. The Python split function splits the word about spaces and returns a list of words or tokens.


---
## Stopword Removal

```yaml
type: "FullSlide"
key: "8595f265a1"
```

`@part1`
We can see commonly occurring words like articles, prepositions etc can be removed because they don't convey anything significant. So we removed these unwanted words called stopwords in the next step.

**Tokenized Text**

```"On","Sunny","Days","I","like","to","play","Cricket" ``` {{1}}


**
Cleaned Text** 
(after stopword removal & converting to lowercase)



```"sunny","days","like,"play","cricket"```{{3}}


`@script`
We can see commonly occurring words like articles, prepositions etc can be removed because they don't convey anything significant. These are called stopwords. This helps to keep vocabulary size small and relevant.

You can see the cleaned text after stopword removal in this example.


---
## NLTK Stopwords

```yaml
type: "FullCodeSlide"
key: "a691d8f039"
```

`@part1`
NLTK comes with a predefined set of Stopwords.
```
from nltk.corpus import stopwords

set(stopwords.words('english'))
```
{{1}}

Some sample stopwords

{‘ourselves’, ‘hers’, ‘between’, ‘yourself’, ‘but’, ‘again’, ‘there’, ‘about’, ‘once’.....}  {{2}}


`@script`
NLTK package also comes with a predefined set of stopwords. You can add your own stopwords as well.

You can import stopwords from nltk corpus and make a Python set using them.
Some sample stopwords are shown here.


---
## Stemming

```yaml
type: "FullSlide"
key: "b2131b4271"
```

`@part1`
Many words(such as verbs) are derived from same base word, and we can reduce all derived/inflected forms of the word into base form in order to reduce total of unique features(words) for classification. This step is called stemming.

> John is the **tallest** guy in the class. He is 6 feet **tall** and 2 feet **taller** than me  {{1}}


Here "tall", "taller", "tallest" all represent the similar context. So it better to reduce of these words into base form word - **tall.** {{2}}


`@script`
Many words(such as verbs) are derived from same base word, and we can reduce all derived/inflected forms of the word into base form in order to reduce total of unique features(words) for classification. This step is called stemming. 
Check the following example.
Here "tall", "taller", "tallest" all represent the similar context. So it better to reduce of these words into base form word.


---
## Building a Common Vocabulary

```yaml
type: "FullSlide"
key: "4073079439"
```

`@part1`
After cleaning all documents, we can filter out all unique words which can be used as features. We combine these words into a single list called Vocabulary.

- Each word in the vocabulary is assigned a unique index, which represents the index of the word in the feature vector and frequency of the word as it value. {{1}}

- The size of vocabulary is denoted by |V| and is also the size of feature vector for each word. {{2}}


`@script`
We can now filter out all unique words which can be used as features. We combine these words into a single list called Vocabulary, and each word is associated with a unique index.

The size of vocabulary is denoted by |V| and is also the size of feature vector for each word.


---
## Vectorization

```yaml
type: "FullSlide"
key: "de50e1536e"
```

`@part1`
This is the last step of the pipeline. We use Vocabulary look up table to convert text into numerical feature vectors.

**Frequency Based Feature Vectors** 
For each document we construct a feature vector having length equal to vocab size, where ith element in the vector represents frequency of ith word from a vocab.{{1}} 

> [ 1 0 3 4 2 0 0 0 1 0 ..........V ] {{2}}



We will use sklearn's **count vectorizer class** for the above purpose.{{3}}

```
from sklearn.feature_extraction.text import CountVectorizer 
```
{{4}}


`@script`
Final step of the pipeline is vectorization. We use Vocabulary table to convert text into numerical feature vectors. For each document we construct a feature vector having length equal to vocab size, where ith element in the vector represents frequency of ith word from a vocab. We will use sklearn count vectorizer class later for this purpose.


---
## Let's practice

```yaml
type: "FinalSlide"
key: "0e2de7bb86"
```

`@script`
So let us put things into practice now!

