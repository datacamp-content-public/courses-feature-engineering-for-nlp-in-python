---
title: Insert title here
key: 16bb4f2b1cecaf5146a69e37564e3166

---
## Bag of Words Pipeline

```yaml
type: "TitleSlide"
key: "639ff16d6a"
```

`@lower_third`

name: Prateek Narang
title: Machine Learning Researcher


`@script`
In the part we are going to learn about Bag Of Words Model, which is popular concept in Natural Language Processing. We will go through various steps involved to build a Bag of Words Pipeline for converting our text data into numerical features.


---
## Understanding Bag of Words(BOW) Model


```yaml
type: "FullSlide"
key: "14ea8de340"
```

`@part1`
Bag of Words Model is way of representing a document as collection of its words. Frequency of occurrence of each word acts helps to define the feature vector for the sentence/document.

**Assumption **- Order of occurrence of each single word in the sentence is not important! {{1}}


**Applications** {{2}}

- Classifying Text/Documents{{3}}
- Clustering Similar Documents{{4}}
- Sentiment/Tweet Analysis{{5}}
- Information Retrieval{{6}}


`@script`
In Bag of Words Model we represent a document as collection of its words. 
Frequency of occurrence of each word acts helps to define the feature vector for the sentence/document.

The model assumes that order of occurrence of each single word in the sentence is not important.

Applications include Classifying & Clustering Documents, Information Retrieval and more.


---
## Let us see one Example

```yaml
type: "FullSlide"
key: "03fc4ee4df"
```

`@part1`
Suppose we got few phone product reviews, and we want to predict whether the review is positive or negative.

**Review 1** {{1}}
> I just **loved** the iPhone. The battery backup is very **good**, and display is excellent too. {{2}}

**Review 2**{{3}}
> 
This phone works very **good**, display is nice and build is also **good**. I **love **Apple devices.{{4}}


`@script`
Suppose we got few phone product reviews, and we want to predict whether the review is positive or negative.
Here are the reviews.
Both reviews contains words like “good”,”excellent”,”loved” which can be treated as features.


---
## Bag of Words Assumption looks reasonable!

```yaml
type: "FullSlide"
key: "a2b7338bcb"
```

`@part1`
- The words "love", "loved", "good" appear at different indices but convey the same positive sentiment about the product. {{1}}
 
- So,BOW claims the feature vectors we are going to construct will be related to word frequencies and local positioning of words will be irrelevant.{{2}}


`@script`
The words "love", "loved", "good" appear at different indices but convey the same positive sentiment about the product. So,BOW claims the feature vectors we are going to construct will be related to word frequencies and local positioning of words will be irrelevant.


---
## Potential Problems with Assumption

```yaml
type: "FullSlide"
key: "1b84ad4817"
```

`@part1`
**Handling Negation**

Consider the following review  
  

> I didn't **like** the phone. The battery was not **good** 

Unigram Features - "good" ,"like", "phone" etc
 
**Solution**


Two Consecutive words "didn't_like", "not_good" can be used as features instead of single word. These are called **Bigrams** features.


`@script`
But there are certain problems with this assumption. You can see
the given review it contains features like "like", "good" but the overall sentiment is negative about the product. 
The way to handle such negation is to treat two consecutive words as a single feature. These are called bigrams.


---
## Bag of Words Pipeline

```yaml
type: "FullSlide"
key: "95e8e6b186"
```

`@part1`
Our objective now is to convert a text document into numerical features. A common bag of words pipeline includes following steps.


**Steps Involved**
- Data Collection(Corpus) {{1}}
- Tokenization & Stopword Removal {{2}}
- Stemming/Lemmatization {{3}}
- Building a Common Vocab {{4}}
- Vectorization {{5}}
- Classification or Clustering  {{6}}


`@script`
So you have learned what Bag of Words Model is.Now its time to see steps involved in building a pipeline for processing text. We will start with data collection, followed by Tokenization, Stopwords Removal and Stemming, and then building a common vocabulary,finally vectorization before feeding into to our classifier.


---
## Tokenization

```yaml
type: "FullSlide"
key: "b374098c37"
```

`@part1`
Tokenization is process of breaking a document into sentences, and sentences into words.

**Sample Text** {{1}}

On Sunny Days, I like to play Cricket.{{2}}

**Tokenized Text** {{3}}

 
"On","Sunny","Days","I","like","to","play","Cricket" {{4}}


`@script`
Tokenization is process of breaking a document into sentences, and sentences into words. Let us break this sample text into words.


---
## ```split()``` Function

```yaml
type: "FullCodeSlide"
key: "68130276a0"
disable_transition: false
```

`@part1`
Sample Text

You can use Python's split function to do the splitting.

```
l = "On Sunny Days, I like to play Cricket."
l = l.split()
``` {{1}}
Tokenized Text

 ```
"On","Sunny","Days","I","like","to","play","Cricket"
```{{2}}


`@script`



---
## Stopword Removal

```yaml
type: "FullSlide"
key: "8595f265a1"
```

`@part1`
We can see commonly occuring words like articles, prepositions, determines etc can be removed because they don't convey anything signifcant. So we removed these unwanted words called stopwords in the next step.

Tokenized Text

> 
"On","Sunny","Days","I","like","to","play","Cricket"
Cleaned Text


(after stopword removal & converting to lowercase)

> 
"sunny","days","like,"play","cricket"


`@script`



---
## NLTK Stopwords

```yaml
type: "FullCodeSlide"
key: "a691d8f039"
```

`@part1`
NLTK comes with a predefined set of Stopwords.
```
from nltk.corpus import stopwords

set(stopwords.words('english'))
```
{{1}}

Some sample stopwords

{‘ourselves’, ‘hers’, ‘between’, ‘yourself’, ‘but’, ‘again’, ‘there’, ‘about’, ‘once’.....}  {{2}}


`@script`
-


---
## Stemming

```yaml
type: "FullSlide"
key: "b2131b4271"
```

`@part1`
Many words(such as verbs) are derived from same base word, and we can reduce all derived/inflected forms of the word into base form in order to reduce total of unique features(words) for classification. This step is called stemming.

> 
John is the **tallest** guy in the class. He is 6 feet **tall** and 2 feet **taller** than me.  

Here "tall", "taller", "tallest" all represent the similar context. So it better to reduce of these words into base form word - **tall.**


`@script`



---
## Building a Common Vocabulary

```yaml
type: "FullSlide"
key: "4073079439"
```

`@part1`
After cleaning all documents, we can filter out all unique words which can be used as features. We combine these words into a single list called Vocabulary.

- Each word in the vocabulary is assigned a unique index, which represents the index of the word in the feature vector and frequency of the word as it value.

- The size of vocabulary is denoted by |V| and is also the size of feature vector for each word.


`@script`



---
## Vectorization

```yaml
type: "FullSlide"
key: "de50e1536e"
```

`@part1`
This is the last step of the pipeline. We use Vocabulary look up table to convert text into numerical feature vectors.

**Frequency Based Feature Vectors** 
For each word we construct a feature vector having length equal to vocab size, where ith element in the vector represents frequency of ith word from a vocab. 

> [ 1 0 3 4 2 0 0 0 1 0 ..........V ]



We will use sklearn's **count vectorizer class** for the above purpose.{{1}}

```
from sklearn.feature_extraction.text import CountVectorizer 
```
{{2}}


`@script`



---
## Let's practice

```yaml
type: "FinalSlide"
key: "0e2de7bb86"
```

`@script`


